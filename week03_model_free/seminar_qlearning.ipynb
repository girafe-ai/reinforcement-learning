{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXq0vWvMPXuQ"
   },
   "source": [
    "## Q-learning (3 points)\n",
    "\n",
    "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
    "\n",
    "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4pyMllfPXuf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcBBQXRxPXuh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly. \n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list). \n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1rmlrTtPXul"
   },
   "source": [
    "### Try it on taxi\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g0XkFVCPXum"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FZzpxtUPXuo"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfbS1qIzPXup"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, truncated, _ = env.step(a)\n",
    "\n",
    "        agent.update(s, a, r, next_s)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB7igYkJPXur",
    "outputId": "fd40a864-7321-4a93-8b93-a2525ca3cd72"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srF2ZK8ZPXuw"
   },
   "source": [
    "# Binarized state spaces\n",
    "\n",
    "Use agent to train efficiently on `CartPole-v1`. This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
    "\n",
    "The simplest way is to use `round(x, n_digits)` (or `np.round`) to round a real number to a given amount of digits. The tricky part is to get the `n_digits` right for each state to train effectively.\n",
    "\n",
    "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlLPudHmPXuz"
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return gym.make('CartPole-v1', render_mode=\"rgb_array\").env  # .env unwraps the TimeLimit wrapper\n",
    "\n",
    "env = make_env()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state: %s\" % (env.reset()[0]))\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBfQ0ueOPXu1"
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4cuF3Z2PXu3"
   },
   "outputs": [],
   "source": [
    "def visualize_cartpole_observation_distribution(seen_observations):\n",
    "    seen_observations = np.array(seen_observations)\n",
    "    \n",
    "    # The meaning of the observations is documented in\n",
    "    # https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py\n",
    "\n",
    "    f, axarr = plt.subplots(2, 2, figsize=(16, 9), sharey=True)\n",
    "    for i, title in enumerate(['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity At Tip']):\n",
    "        ax = axarr[i // 2, i % 2]\n",
    "        ax.hist(seen_observations[:, i], bins=20)\n",
    "        ax.set_title(title)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n",
    "        ax.grid()\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLPsHYj9PXu4",
    "outputId": "0830e905-b15c-4fe7-d4b3-576ed3bcaa2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seen_observations = []\n",
    "for _ in range(1000):\n",
    "    seen_observations.append(env.reset()[0])\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, truncated, _ = env.step(env.action_space.sample())\n",
    "        seen_observations.append(s)\n",
    "\n",
    "visualize_cartpole_observation_distribution(seen_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRD_w3LmPXu5"
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtcWYCrOPXu5"
   },
   "outputs": [],
   "source": [
    "from gymnasium.core import ObservationWrapper\n",
    "\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def observation(self, state):\n",
    "        # Hint: you can do that with round(x, n_digits).\n",
    "        # You may pick a different n_digits for each dimension.\n",
    "        dec = [1, 1, 2, 1]\n",
    "        state = [np.round(state_i, dec[i]) for i, state_i in enumerate(state)]\n",
    "        \n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2icHHYCxPXu7"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(make_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g7DfVsUPXu8"
   },
   "outputs": [],
   "source": [
    "seen_observations = []\n",
    "for _ in range(1000):\n",
    "    seen_observations.append(env.reset()[0])\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, truncated, _ = env.step(env.action_space.sample())\n",
    "        seen_observations.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "visualize_cartpole_observation_distribution(seen_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkeK0W84PXu_"
   },
   "source": [
    "## Learn binarized policy\n",
    "\n",
    "Now let's train a policy that uses binarized state space.\n",
    "\n",
    "__Tips:__\n",
    "\n",
    "* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n",
    "* If your binarization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce epsilon decay or change binarization. In practice we found that this kind of mistake is rather frequent.\n",
    "* If your binarization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n",
    "* **Start with a coarse binarization** and make it more fine-grained if that seems necessary.\n",
    "* Having $10^3$–$10^4$ distinct states is recommended (`len(agent._qvalues)`), but not required.\n",
    "* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n",
    "\n",
    "A reasonable agent should attain an average reward of at least 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA_pqyStPXvI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6ba0mPePXvK"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz_-uzRYPXvL"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "epsilons = []\n",
    "\n",
    "for i in range(10000):\n",
    "    reward = play_and_train(env, agent)\n",
    "    rewards.append(reward)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    \n",
    "    # OPTIONAL: <YOUR CODE: adjust epsilon>\n",
    "    agent.epsilon *= (1 - 1e-4)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        rewards_ewma = moving_average(rewards)\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.plot(rewards, label='rewards')\n",
    "        plt.plot(rewards_ewma, label='rewards ewma@100')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.title('eps = {:e}, rewards ewma@100 = {:.1f}'.format(agent.epsilon, rewards_ewma[-1]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeuQ7RY5PXvL"
   },
   "outputs": [],
   "source": [
    "print('Your agent has learned {} Q-values.'.format(len(agent._qvalues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def play_and_visualize(env, agent, t_max=10**4):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, truncated, _ = env.step(a)\n",
    "        clear_output(True)\n",
    "        plt.imshow(env.render())\n",
    "        plt.show()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"total_reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Binarizer(make_env())\n",
    "play_and_visualize(env, agent)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "seminar_qlearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
