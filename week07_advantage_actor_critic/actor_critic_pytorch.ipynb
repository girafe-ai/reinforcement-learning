{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "# Actor Critic in PyTorch\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v1` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjkAlO8Qi_Xm"
   },
   "source": [
    "Acknowledgements for this great notebook to the [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "1b0moDpxi2SW"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "7UYczVTli2Sb"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98zhewLFi2Sd"
   },
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "XPKYrIlai2Sf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x148ece0d0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "sY2THBWfi2Sl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "8_pYr7PZi2Sn"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Your code goes here...\n",
    "\n",
    "model = Policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    \n",
    "    logits, state_value = model.forward(torch.tensor(states))\n",
    "    probs = nn.Softmax(dim=1)(logits)\n",
    "    return probs.detach().numpy(), state_value.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "Obkl_jCii2Sv"
   },
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas, _ = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000, video_recorder=None):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards, state_values = [], [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs, _ = predict_probs(np.array([s]))\n",
    "        action_probs = action_probs[0]\n",
    "        \n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "        if video_recorder is not None:\n",
    "            video_recorder.capture_frame()\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cumulative_rewards = np.zeros(len(rewards))\n",
    "    total_reward = 0\n",
    "    for i in range(len(cumulative_rewards) - 1, -1, -1):\n",
    "        total_reward = rewards[i] + gamma * total_reward\n",
    "        cumulative_rewards[i] = total_reward\n",
    "    \n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "2DX39wcUi2S3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "    \n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits, state_values = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    state_values = state_values.reshape(-1)\n",
    "    # Your code goes here...\n",
    "    \n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -(probs * log_probs).sum(dim=1).mean()\n",
    "    loss = - torch.mean(log_probs_for_actions * cumulative_returns) - entropy * entropy_coef\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-WWsbl5i2TE"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "ckHj5sXBi2TE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-499.200\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-484.000\n",
      "mean reward:-498.400\n",
      "mean reward:-489.200\n",
      "mean reward:-496.600\n",
      "mean reward:-476.100\n",
      "mean reward:-489.000\n",
      "mean reward:-496.800\n",
      "mean reward:-497.700\n",
      "mean reward:-494.000\n",
      "mean reward:-485.200\n",
      "mean reward:-485.400\n",
      "mean reward:-500.000\n",
      "mean reward:-475.400\n",
      "mean reward:-487.800\n",
      "mean reward:-491.200\n",
      "mean reward:-500.000\n",
      "mean reward:-460.300\n",
      "mean reward:-480.400\n",
      "mean reward:-488.600\n",
      "mean reward:-478.100\n",
      "mean reward:-459.900\n",
      "mean reward:-432.200\n",
      "mean reward:-475.600\n",
      "mean reward:-426.400\n",
      "mean reward:-440.200\n",
      "mean reward:-471.300\n",
      "mean reward:-359.300\n",
      "mean reward:-387.400\n",
      "mean reward:-423.100\n",
      "mean reward:-361.900\n",
      "mean reward:-396.000\n",
      "mean reward:-374.700\n",
      "mean reward:-307.300\n",
      "mean reward:-292.900\n",
      "mean reward:-313.600\n",
      "mean reward:-331.000\n",
      "mean reward:-393.600\n",
      "mean reward:-401.100\n",
      "mean reward:-380.600\n",
      "mean reward:-319.600\n",
      "mean reward:-338.100\n",
      "mean reward:-275.100\n",
      "mean reward:-302.300\n",
      "mean reward:-322.300\n",
      "mean reward:-273.200\n",
      "mean reward:-340.700\n",
      "mean reward:-336.100\n",
      "mean reward:-296.200\n",
      "mean reward:-284.100\n",
      "mean reward:-296.200\n",
      "mean reward:-302.300\n",
      "mean reward:-390.000\n",
      "mean reward:-383.200\n",
      "mean reward:-355.700\n",
      "mean reward:-356.700\n",
      "mean reward:-355.700\n",
      "mean reward:-312.500\n",
      "mean reward:-336.600\n",
      "mean reward:-354.200\n",
      "mean reward:-317.000\n",
      "mean reward:-266.900\n",
      "mean reward:-202.600\n",
      "mean reward:-196.800\n",
      "mean reward:-209.200\n",
      "mean reward:-196.500\n",
      "mean reward:-219.800\n",
      "mean reward:-212.600\n",
      "mean reward:-186.500\n",
      "mean reward:-216.300\n",
      "mean reward:-202.300\n",
      "mean reward:-212.700\n",
      "mean reward:-206.400\n",
      "mean reward:-197.600\n",
      "mean reward:-229.500\n",
      "mean reward:-209.500\n",
      "mean reward:-212.100\n",
      "mean reward:-179.400\n",
      "mean reward:-226.100\n",
      "mean reward:-179.300\n",
      "mean reward:-190.200\n",
      "mean reward:-169.900\n",
      "mean reward:-172.900\n",
      "mean reward:-169.400\n",
      "mean reward:-172.100\n",
      "mean reward:-164.800\n",
      "mean reward:-148.100\n",
      "mean reward:-156.900\n",
      "mean reward:-175.700\n",
      "mean reward:-153.800\n",
      "mean reward:-191.900\n",
      "mean reward:-156.100\n",
      "mean reward:-145.800\n",
      "mean reward:-146.500\n",
      "mean reward:-141.500\n",
      "mean reward:-132.400\n",
      "mean reward:-153.800\n",
      "mean reward:-156.000\n",
      "mean reward:-181.200\n",
      "mean reward:-179.300\n",
      "mean reward:-255.800\n",
      "mean reward:-232.800\n",
      "mean reward:-185.700\n",
      "mean reward:-218.700\n",
      "mean reward:-184.300\n",
      "mean reward:-187.200\n",
      "mean reward:-167.200\n",
      "mean reward:-157.200\n",
      "mean reward:-159.800\n",
      "mean reward:-204.000\n",
      "mean reward:-147.300\n",
      "mean reward:-133.000\n",
      "mean reward:-134.500\n",
      "mean reward:-117.400\n",
      "mean reward:-151.700\n",
      "mean reward:-133.800\n",
      "mean reward:-163.500\n",
      "mean reward:-145.600\n",
      "mean reward:-133.200\n",
      "mean reward:-160.000\n",
      "mean reward:-146.400\n",
      "mean reward:-186.100\n",
      "mean reward:-141.800\n",
      "mean reward:-291.300\n",
      "mean reward:-290.900\n",
      "mean reward:-276.900\n",
      "mean reward:-335.700\n",
      "mean reward:-455.500\n",
      "mean reward:-426.000\n",
      "mean reward:-253.500\n",
      "mean reward:-358.000\n",
      "mean reward:-319.100\n",
      "mean reward:-362.800\n",
      "mean reward:-288.800\n",
      "mean reward:-257.600\n",
      "mean reward:-208.800\n",
      "mean reward:-237.500\n",
      "mean reward:-306.400\n",
      "mean reward:-222.000\n",
      "mean reward:-317.700\n",
      "mean reward:-269.500\n",
      "mean reward:-197.400\n",
      "mean reward:-213.000\n",
      "mean reward:-195.400\n",
      "mean reward:-177.200\n",
      "mean reward:-177.200\n",
      "mean reward:-139.200\n",
      "mean reward:-153.600\n",
      "mean reward:-136.100\n",
      "mean reward:-137.200\n",
      "mean reward:-133.000\n",
      "mean reward:-151.600\n",
      "mean reward:-146.300\n",
      "mean reward:-186.100\n",
      "mean reward:-190.400\n",
      "mean reward:-183.800\n",
      "mean reward:-166.400\n",
      "mean reward:-153.700\n",
      "mean reward:-230.200\n",
      "mean reward:-132.500\n",
      "mean reward:-180.400\n",
      "mean reward:-165.500\n",
      "mean reward:-154.000\n",
      "mean reward:-232.500\n",
      "mean reward:-182.700\n",
      "mean reward:-140.800\n",
      "mean reward:-108.600\n",
      "mean reward:-128.600\n",
      "mean reward:-110.700\n",
      "mean reward:-101.900\n",
      "mean reward:-104.000\n",
      "mean reward:-112.100\n",
      "mean reward:-100.700\n",
      "mean reward:-103.900\n",
      "mean reward:-108.000\n",
      "mean reward:-102.000\n",
      "mean reward:-94.300\n",
      "mean reward:-111.200\n",
      "mean reward:-93.400\n",
      "mean reward:-95.800\n",
      "mean reward:-103.400\n",
      "mean reward:-106.900\n",
      "mean reward:-102.100\n",
      "mean reward:-102.000\n",
      "mean reward:-136.400\n",
      "mean reward:-185.300\n",
      "mean reward:-342.700\n",
      "mean reward:-317.000\n",
      "mean reward:-336.500\n",
      "mean reward:-400.900\n",
      "mean reward:-451.500\n",
      "mean reward:-486.700\n",
      "mean reward:-475.800\n",
      "mean reward:-442.800\n",
      "mean reward:-469.000\n",
      "mean reward:-486.800\n",
      "mean reward:-464.400\n",
      "mean reward:-480.200\n",
      "mean reward:-485.200\n",
      "mean reward:-481.600\n",
      "mean reward:-478.600\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-499.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-498.400\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n",
      "mean reward:-500.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtrain_on_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# generate new sessions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean reward:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(rewards)))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m800\u001b[39m:\n",
      "Cell \u001b[0;32mIn[155], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m [train_on_session(\u001b[38;5;241m*\u001b[39m\u001b[43mgenerate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m, entropy_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]  \u001b[38;5;66;03m# generate new sessions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean reward:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(rewards)))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m800\u001b[39m:\n",
      "Cell \u001b[0;32mIn[149], line 12\u001b[0m, in \u001b[0;36mgenerate_session\u001b[0;34m(env, t_max, video_recorder)\u001b[0m\n\u001b[1;32m      8\u001b[0m s, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t_max):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# action probabilities array aka pi(a|s)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     action_probs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m action_probs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Sample action with given probabilities.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[147], line 9\u001b[0m, in \u001b[0;36mpredict_probs\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPredict action probabilities given states.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m:param states: numpy array of shape [batch, state_shape]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m:returns: numpy array of shape [batch, n_actions]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# convert states, compute logits, use softmax to get probability\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m logits, state_value \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(logits)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), state_value\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[0;32mIn[146], line 14\u001b[0m, in \u001b[0;36mPolicy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine2(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine3(x))\n",
      "File \u001b[0;32m~/reinforcement-learning/rl_mipt_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/reinforcement-learning/rl_mipt_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    rewards = [train_on_session(*generate_session(env, t_max=500), entropy_coef=1e-3) for _ in range(10)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    \n",
    "    if np.mean(rewards) > -200:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "cAoTsq4Pi2TF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/video.mp4.\n",
      "Moviepy - Writing video videos/video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"videos\").mkdir(exist_ok=True)\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\").env\n",
    "video_recorder = VideoRecorder(env, path=\"videos/video.mp4\")\n",
    "sessions = [generate_session(env, video_recorder=video_recorder, t_max=100000) for _ in range(1)]\n",
    "video_recorder.close()\n",
    "video_recorder.enabled = False\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "IgcqBKtBi2TG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/video.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "reinforce_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
