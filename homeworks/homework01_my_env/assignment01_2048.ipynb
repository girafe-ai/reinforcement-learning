{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deadline**: 30.10.2024 19:00 (GMT+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Implement 2048 environment (6 points)\n",
    "\n",
    "In this homework our goal is implement and solve [2048 game](https://en.wikipedia.org/wiki/2048_(video_game)). If you are not familiar with it, it's highly recommended to play some games (e.g. [here](https://2048game.com/)).\n",
    "\n",
    "One of the most interesting question that you probably will ask yourself during solving this task is \"How to design reward function here?\". You can use the [reward function from the origin game](https://en.wikipedia.org/wiki/2048_(video_game)#:~:text=The%20user's%20score%20starts%20at,that%20to%20reach%20higher%20scores.) or you can use it as inspiration for developing your own reward function. Think about what is \"good\" reward function in this context. \n",
    "\n",
    "Implementation **must be consistent** with gymnasium interfaces. Keep in mind that all tests passed is requirement for achiving non-zero score for this task.\n",
    "\n",
    "Below you can find tests that may help you with implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from env import Game2048Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0\n",
      "Highest: 2\n",
      "[[0 2 0 0]\n",
      " [0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env(render_mode='ansi')\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env.get_board().shape == (4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left = np.array([[2, 0, 2, 4], [2, 2, 2, 2], [4, 8, 0, 16], [8, 16, 4, 0]])\n",
    "answer = np.array([[4, 4, 0, 0], [4, 4, 0, 0], [4, 8, 16, 0], [8, 16, 4, 0]])\n",
    "\n",
    "env.set_board(test_left)\n",
    "score = env.move(3)\n",
    "assert np.allclose(env.get_board(), answer)\n",
    "assert np.allclose(score, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Implement CEM (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, your goal is to train the CEM algorithm to solve your own environment. Try any tricks you learned in the webinars. You will solve the task if your solution can reach 1024 as a max tile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_mipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
